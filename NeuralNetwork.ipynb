{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "PyAWQkAWvwFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB1WYiGgTb8I"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import sklearn as ski\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.neural_network import MLPClassifier as multilp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
        "                 'machine-learning-databases/wine/wine.data', names=(\n",
        "                                            'Class', \n",
        "                                            'Alcohol', \n",
        "                                            'Malic acid',\n",
        "                                            'Ash', \n",
        "                                            'Alcalinity of ash',\n",
        "                                            'Magnesium', \n",
        "                                            'Total phenols',\n",
        "                                            'Flavanoids', \n",
        "                                            'Nonflavanoid phenols',\n",
        "                                            'Proanthocyanins', \n",
        "                                            'Color intensity',\n",
        "                                            'Hue', \n",
        "                                            'OD280/OD315 of diluted wines',\n",
        "                                            'Proline' ))\n",
        "#training_num =int( len(df) * .6 )\n",
        "#classif = df.iloc[1:training_num,0]\n",
        "#x_train = df.loc[1:training_num,df.columns != 'Class']\n",
        "#classif\n",
        "classif = df['Class']\n",
        "df = df.drop(['Class'],axis = 1)\n",
        "\n",
        "#for col in range(len(df.columns)):\n",
        " # sum = df.iloc[:,col].sum()\n",
        "  #print(sum)\n",
        "  #for row in range(len(df)):\n",
        "   # df.iloc[row,col] = df.iloc[row,col] / sum \n",
        "\n",
        "for col in range(len(df.columns)):\n",
        "  mean = df.iloc[:,col].sum()/len(df)\n",
        "  std = df.iloc[:,col].std()\n",
        "  for row in range(len(df)):\n",
        "   df.iloc[row,col] = (df.iloc[row,col] - mean) / std\n",
        "df['Class'] = classif\n",
        "np.random.seed(99)\n",
        "x_train, x_test, y_train, y_test = train_test_split(df, df['Class'], test_size=0.2)\n",
        "learning_rate = .2\n",
        "#for i in range(len(x_train)):\n",
        " # print(y_train.iloc[i])\n",
        "# Dropping targets from training set, we do not want to train over targets\n",
        "#print(np.shape(x_train),np.shape(x_test),np.shape(y_train),np.shape(y_test))\n",
        "x_train = x_train.drop(['Class'], axis=1)\n",
        "x_test = x_test.drop(['Class'], axis=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Number of hidden neurons is 6 so # of weights from all input nodes to hidden neurons is 13 * 6 \n",
        "# Number of output neurons = 3, 1 for each class \n",
        "from numpy.random import default_rng\n",
        "\n",
        "np.random.seed(775) \n",
        "#100 good\n",
        "#71 even better\n",
        "#775 amazing \n",
        "output = [0,0,0]\n",
        "\n",
        "class HiddenLayer():\n",
        "  hiddenNodes = []\n",
        "  bias = -1\n",
        "  def __init__(self,numOfHN):\n",
        "    i = 0 \n",
        "    while(i < numOfHN):\n",
        "        hn = HiddenNode() \n",
        "        HiddenLayer.hiddenNodes.append(hn)\n",
        "        i+=1\n",
        "    \n",
        "\n",
        "class HiddenNode:\n",
        "  seed = 0 \n",
        "  seed1 = 400 \n",
        "  pre_af_val = 0 \n",
        "  post_af_val = 0 \n",
        "  inp_to_h_weights = []\n",
        "  h_to_out = []\n",
        "  def __init__(self):\n",
        "    self.inp_to_h_weights =  np.random.uniform(-1,1,size = (1,13))\t#default_rng(HiddenNode.seed).random((13)) #numpy.random.uniform()\t\n",
        "    self.inp_to_h_weights = self.inp_to_h_weights[0][:]\n",
        "    HiddenNode.seed += 1 \n",
        "    self.h_to_out = np.random.uniform(-1,1,size = (1,3)) #default_rng(HiddenNode.seed1).random(3)\n",
        "    self.h_to_out = self.h_to_out[0][:]\n",
        "    HiddenNode.seed1 += 20\n",
        "    #print(self.inp_to_h_weights)\n",
        "    #print(len(self.inp_to_h_weights))\n",
        "    print(self.h_to_out)\n",
        "\n",
        "\n",
        "hl = HiddenLayer(6) #initialize MLP with 1 hidden layer of 6 nodes, randomize input to hidden layer weights (13 weights per hidden neuron), \n",
        "                    #randomize hidden to output weights (3 weights per hidden)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5dYCMJHT0dT",
        "outputId": "39b852a9-26d1-4449-dd7b-bca1371f7ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.14139123 -0.68429059 -0.33354377]\n",
            "[-0.59525376 -0.09417655  0.25755592]\n",
            "[ 0.58984275  0.04925569 -0.88868765]\n",
            "[-0.95431318 -0.01769389  0.44438857]\n",
            "[ 0.999286   -0.85877564 -0.15392371]\n",
            "[ 0.14141774 -0.38462306  0.95804651]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#for each row in training dataframe \n",
        "#     feed forward \n",
        "#     calculate error \n",
        "#     backpropagate \n",
        "\n",
        "for row in range(len(x_train)) : #each row used as training data, fed forward and backpropagation for each row \n",
        "  #multiply each col by corresponding weight \n",
        "  for col in range(len(x_train.columns)): #iterate through columns\n",
        "    for hn in range(len(hl.hiddenNodes)) :#multiply the column value by each weight that is attached to the input node that corresponds to the column \n",
        "                                          #add that value to the pre_af_val for each hidden layer node \n",
        "      hl.hiddenNodes[hn].pre_af_val  +=   (x_train.iloc[row,col] * hl.hiddenNodes[hn].inp_to_h_weights[col])\n",
        "  \n",
        "  #after going through all the columns and calculating pre-activation function value, we use activation function on pre-activation function value to get post-actfunc val \n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "    #print(hl.hiddenNodes[hn].pre_af_val)\n",
        "    hl.hiddenNodes[hn].post_af_val = 1/(1 + np.exp(-(hl.hiddenNodes[hn].pre_af_val + hl.bias)))\n",
        "    #print(\"Post Act Func Val: {post_af}\".format(post_af = hl.hiddenNodes[hn].post_af_val))\n",
        "\n",
        "  #loop through hidden neurons and multiply post-af val by corresponding weights to get output pre-af values \n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "    for o in range(len(output)):\n",
        "      output[o] += (hl.hiddenNodes[hn].h_to_out[o] * hl.hiddenNodes[hn].post_af_val)\n",
        "      \n",
        "  sum1 = 0 \n",
        "  for o in range(len(output)):\n",
        "    output[o] = np.exp(output[o]) \n",
        "    sum1 += output[o]\n",
        "\n",
        "  for o in range(len(output)):\n",
        "    output[o] = output[o]/sum1\n",
        "\n",
        "\n",
        "  #for i in range(len(output)):\n",
        "    #print(\"Output {i}: {output}\".format(i = i, output = output[i])) \n",
        "  #print() \n",
        "\n",
        "  #compute the loss function \n",
        "  sse = 0 \n",
        "  t_output = [0,0,0]\n",
        "  t_output[y_train.iloc[row] - 1] = 1 #create the target output vector \n",
        "  #if(y_train[row] == 1):\n",
        "   # t_output[0] = 1\n",
        "  #elif(y_train[row] == 2):\n",
        "   # t_output[1] = 1\n",
        "  #else:\n",
        "   # t_output[2] = 1 \n",
        "\n",
        "  for o in range(len(output)):\n",
        "    sse += (output[o] - t_output[o])** 2 \n",
        "  sse = sse * .5 \n",
        "\n",
        "  #calculate gradient for 2nd layer weights\n",
        "  #gradient is different for each output neuron \n",
        "\n",
        "  grad_out = [0,0,0]\n",
        "  for o in range(len(output)):\n",
        "    grad_out[o] = (output[o] - t_output[o]) * output[o] * (1 - output[o])\n",
        "  count = 0 \n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "    for i in range(len(hl.hiddenNodes[hn].h_to_out)): \n",
        "      count+= 1 \n",
        "      #print(\"Iteration: {number}\".format(number = count))\n",
        "      if(i < 3):\n",
        "        grad_k = grad_out[0]\n",
        "      elif(i < 6):\n",
        "        grad_k = grad_out[1]\n",
        "      else:\n",
        "        grad_k = grad_out[2]\n",
        "      #print(\"Weight before change: {weight}\".format(weight = hl.hiddenNodes[hn].h_to_out[i]))\n",
        "      hl.hiddenNodes[hn].h_to_out[i] = hl.hiddenNodes[hn].h_to_out[i] - (learning_rate * grad_k * hl.hiddenNodes[hn].post_af_val)\n",
        "      #print(\"Weight after change: {weight}\".format(weight = hl.hiddenNodes[hn].h_to_out[i]))\n",
        "      #print()\n",
        "\n",
        "  #calculate gradient for each hidden layer neuron \n",
        "  grad_h = [0,0,0,0,0,0]\n",
        "  sum_g_weights = [0,0,0,0,0,0]\n",
        "  #need summation of gradient times h_to_output\n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "    for w in range(len(hl.hiddenNodes[hn].h_to_out)):\n",
        "      g = 0 \n",
        "      if(w == 0):\n",
        "        g = grad_out[0]\n",
        "      elif(w == 1):\n",
        "        g = grad_out[1]\n",
        "      else:\n",
        "        g = grad_out[2]\n",
        "      sum_g_weights[hn] += (hl.hiddenNodes[hn].h_to_out[w] * g)\n",
        "\n",
        "\n",
        "  for g in range(len(grad_h)):\n",
        "    grad_h[g] = hl.hiddenNodes[g].post_af_val*(1 - hl.hiddenNodes[g].post_af_val) * sum_g_weights[g]\n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "      for w in range(len(hl.hiddenNodes[hn].inp_to_h_weights)):\n",
        "        hl.hiddenNodes[hn].inp_to_h_weights[w] = hl.hiddenNodes[hn].inp_to_h_weights[w] - learning_rate * grad_h[hn]\n",
        "\n",
        "\n",
        "  #after completing backpropagation, pre and post activation function value and output needs to be reset to zero \n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "    hl.hiddenNodes[hn].pre_af_val = 0 \n",
        "    hl.hiddenNodes[hn].post_af_val = 0 \n",
        "  #output also needs to be reset \n",
        "  for i in range(len(output)):\n",
        "    output[i] = 0 "
      ],
      "metadata": {
        "id": "IfpK6b8VT9qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_count = 0 \n",
        "for row in range(len(x_test)):\n",
        "  for col in range(len(x_test.columns)): #iterate through columns\n",
        "    for hn in range(len(hl.hiddenNodes)) :#multiply the column value by each weight that is attached to the input node that corresponds to the column \n",
        "                                          #add that value to the pre_af_val for each hidden layer node \n",
        "      hl.hiddenNodes[hn].pre_af_val  +=   (x_test.iloc[row,col] * hl.hiddenNodes[hn].inp_to_h_weights[col])\n",
        "  \n",
        "  #after going through all the columns and calculating pre-activation function value, we use activation function on pre-activation function value to get post-actfunc val \n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "    #print(hl.hiddenNodes[hn].pre_af_val)\n",
        "    hl.hiddenNodes[hn].post_af_val = 1/(1 + np.exp(-(hl.hiddenNodes[hn].pre_af_val + hl.bias)))\n",
        "    #print(\"Post Act Func Val: {post_af}\".format(post_af = hl.hiddenNodes[hn].post_af_val))\n",
        "\n",
        "  #loop through hidden neurons and multiply post-af val by corresponding weights to get output pre-af values \n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "    for o in range(len(output)):\n",
        "      output[o] += (hl.hiddenNodes[hn].h_to_out[o] * hl.hiddenNodes[hn].post_af_val)\n",
        "      \n",
        "  sum1 = 0 \n",
        "  #use softmax activation function to get which class\n",
        "  for o in range(len(output)):\n",
        "    output[o] = np.exp(output[o]) \n",
        "    sum1 += output[o]\n",
        "\n",
        "  for o in range(len(output)):\n",
        "    output[o] = output[o]/sum1\n",
        "  ####################################\n",
        "  #print(output)\n",
        "  target = y_test.iloc[row]\n",
        "  maxProb = 0 \n",
        "  class_output = 0 \n",
        "  for i in range(len(output)):\n",
        "    if(output[i] > maxProb):\n",
        "      maxProb = output[i]\n",
        "      class_output = i + 1 \n",
        "  #print(\"Target: \" + str(target))\n",
        "  if(target == class_output):\n",
        "    #print(\"Correct\")\n",
        "    correct_count+=1\n",
        "  \n",
        "  #print()\n",
        "  # Activation function values need to be reset and output needs to be reset\n",
        "  for hn in range(len(hl.hiddenNodes)):\n",
        "    hl.hiddenNodes[hn].pre_af_val = 0 \n",
        "    hl.hiddenNodes[hn].post_af_val = 0 \n",
        "  #output also needs to be reset \n",
        "  for i in range(len(output)):\n",
        "    output[i] = 0 \n",
        "\n",
        "print(\"Correct percentage: \" + str(correct_count / len(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnLSZuYesas5",
        "outputId": "196e81b7-0d00-45f6-dd57-449edac36cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct percentage: 0.6666666666666666\n"
          ]
        }
      ]
    }
  ]
}